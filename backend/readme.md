# Install and setup instructions
0. ensure pipenv is installed: https://pipenv.pypa.io/en/stable/index.html

    ```pip install --user pipenv```

1. edit .env to set up environment variables for LLM GPU offloading
- please refer to [6a. GPU Offloading for llama‐cpp‐python](https://github.com/calebleehy/gxs-sentiment-analysis/wiki/6a.-GPU-Offloading-for-llama%E2%80%90cpp%E2%80%90python)

2. install requirements, NEEDS PYTHON 11

    `cd ./backend && pipenv install [--python [path to python11's python.exe]]`

4. download model

    `pipenv run huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q5_K_M.gguf --local-dir .\model --local-dir-use-symlinks False`

# File structure:
	backend
	├── archived_data: explained below
	│	└── ...
	├── .env: explained below
	├── data: all scripts in src will read and write from this folder
	├── src
	│	├── `reviews_scraping.py`
	│	├── generation: scripts that do LLM inference, refer to each one's docstrings for more info
	│	│	└── ...
	│	├── eda.py
	│	├── Chatgpt_chat_log_recommendation.pdf
	│	├── final_processing.py
	│	├── evaluation.py
	│	└── backend_utils.py
	├── model: model file to be downloaded to here
	├── server: flask app to hand off generated results to frontend 
	│	└── ...
	├── Pipfile: specifies requirements for pipenv
	├── readme.md: the file you are reading now
	└── .gitignore: ignores autogenerated files e.g. `\_\_pycache__`, `Pipfile.lock`, etc

archived_data: contains all scraped data and generated results from our initial run. You may treat this as a canonical list of all the files that *should* be generated over the course of running this project for yourself

`.env`: contains environment variables related to installation of llama_cpp_python library, in particular GPU offloading. modify these according to your system. 
- please refer to [6a. GPU Offloading for llama‐cpp‐python](https://github.com/calebleehy/gxs-sentiment-analysis/wiki/6a.-GPU-Offloading-for-llama%E2%80%90cpp%E2%80%90python)

# Script run order:
    cd ./src
	pipenv run [script].py
1. [review_scraping](src/review_scraping.py)
2. [generation/sentiment](src/generation/sentiment.py)
3. [generation/service](src/generation/service.py)
4. [generation/issue](src/generation/issue.py)
5. [merge](src/merge.py)
6. [eda](src/eda.py)
   - [evaluation](src/evaluation.py) to generate comparisons to RoBERTA
7. [generation/recommendation](src/generation/recommendation.py)
8. [tables_for_dashboard](src/tables_for_dashboard.py)

# Scrapped features: 
## jupyterlab: 
Causes cross-platform dependency conflicts that we ran out of time to resolve. in particular the pywin32 and pywinpty sub-dependencies. If you do manage to get it working, here is what you can do:

	OPTIONAL if you want a more sandbox environment to play around: 
	- install, start jupyterlab
	> pipenv run python -m jupyterlab
	- start local inference server
	> pipenv run python -m llama_cpp.server --model .\model\mistral-7b-instruct-v0.2.Q5_K_M.gguf

