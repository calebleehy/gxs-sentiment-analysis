# Install and setup instructions
0. ensure pipenv is installed: https://pipenv.pypa.io/en/stable/index.html
> pip install --user pipenv

1. edit .env to set up environment variables for LLM GPU offloading
- <ADD LINK TO RELEVANT GITHUB WIKI PAGE LATER WHEN DONE>

2. install requirements, NEEDS PYTHON 11
> cd .\backend && pipenv install [--python <path to python11's python.exe>]

3. download model
> pipenv run huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q5_K_M.gguf --local-dir .\model --local-dir-use-symlinks False

# File structure:
	backend
	├── archived_data: explained below
	│	└── ...
	├── `.env`: explained below
	├── `data`: all scripts in src will read and write from this folder
	├── `src`
	│	├── `reviews_scraping.py`
	│	├── generation: scripts that do LLM inference, will have its own README
	│	│	└── ...
	│	├── `eda.py`
	│	├── Chatgpt_chat_log_recommendation.pdf
	│	├── `final_processing.py`
	│	├── `evaluation.py`
	│	└── `backend_utils.py`
	├── model: model file to be downloaded to here
	├── server: flask app to hand off generated results to frontend 
	│	└── ...
	├── `Pipfile`: specifies requirements for pipenv
	├── `readme.md`: the file you are reading now
	└── `.gitignore`: ignores autogenerated files e.g. `\_\_pycache__`, `Pipfile.lock`, etc

archived_data: contains all scraped data and generated results from our initial run. You may treat this as a canonical list of all the files that *should* be generated over the course of running this project for yourself

`.env`: contains environment variables related to installation of llama_cpp_python library, in particular GPU offloading. modify these according to your system. <ADD LINK TO RELEVANT GITHUB WIKI PAGE LATER WHEN DONE>

# Scrapped features: 
## jupyterlab: 
Causes cross-platform dependency conflicts that we ran out of time to resolve. in particular the pywin32 and pywinpty sub-dependencies. If you do manage to get it working, here is what you can do:

	OPTIONAL if you want a more sandbox environment to play around: 
	- install, start jupyterlab
	> pipenv run python -m jupyterlab
	- start local inference server
	> pipenv run python -m llama_cpp.server --model .\model\mistral-7b-instruct-v0.2.Q5_K_M.gguf

